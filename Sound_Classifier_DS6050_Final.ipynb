{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mBSFTJ5M_z-H"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import torch\n",
        "from torch.cuda import manual_seed_all\n",
        "from torch.backends import cudnn\n",
        "import matplotlib as mpl\n",
        "from matplotlib import pyplot as plt\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3uDKfjRHMuxk"
      },
      "outputs": [],
      "source": [
        "# pre spectrogram augmentations\n",
        "# these are examples and can be changed based on domain knowledge\n",
        "\n",
        "time_stretch = T.TimeStretch()\n",
        "def stretch_waveform(waveform, rate=1.2):\n",
        "    # `rate > 1.0` speeds up, `rate < 1.0` slows down\n",
        "    return time_stretch(waveform, rate)\n",
        "\n",
        "pitch_shift = T.PitchShift(sample_rate=44100, n_steps=2)  # Shift up by 2 semitones\n",
        "def shift_pitch(waveform, sample_rate):\n",
        "    return pitch_shift(waveform)\n",
        "\n",
        "def scale_volume(waveform, factor=1.5):\n",
        "    return waveform * factor  # Amplifies waveform by factor\n",
        "\n",
        "def crop_waveform(waveform, crop_size):\n",
        "    start = torch.randint(0, max(1, waveform.size(-1) - crop_size), (1,)).item()\n",
        "    return waveform[:, start:start + crop_size]\n",
        "\n",
        "def apply_reverb(waveform):\n",
        "    reverb = T.Reverberate()\n",
        "    return reverb(waveform)\n",
        "\n",
        "def time_shift(waveform, shift):\n",
        "    return torch.roll(waveform, shifts=shift, dims=-1)\n",
        "\n",
        "def add_noise(waveform, noise_level=0.005):\n",
        "    noise = torch.randn_like(waveform) * noise_level\n",
        "    return waveform + noise\n",
        "\n",
        "# Augment on-the-fly stochastically\n",
        "# again these are just examples and do not necessarily utilize the methods above\n",
        "def augment_waveform(data):\n",
        "    waveform, sample_rate = data\n",
        "    if torch.rand(1).item() > 0.5:\n",
        "        waveform += torch.randn_like(waveform) * 0.005\n",
        "    if torch.rand(1).item() > 0.5:\n",
        "        waveform = torch.roll(waveform, shifts=torch.randint(-5000, 5000, (1,)).item(), dims=-1)\n",
        "    if torch.rand(1).item() > 0.5:\n",
        "        waveform *= torch.FloatTensor(1).uniform_(0.8, 1.5).item()\n",
        "    return waveform, sample_rate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ww8OMV8nNZcf"
      },
      "outputs": [],
      "source": [
        "# Create a MelSpectrogram transformation\n",
        "mel_spectrogram_transform = T.MelSpectrogram(\n",
        "    sample_rate=44100,         # Default sample rate, change if needed\n",
        "    n_fft=1024,                # Number of FFT bins\n",
        "    hop_length=512,            # Hop length between windows\n",
        "    n_mels=64                  # Number of Mel bands\n",
        ")\n",
        "\n",
        "def waveform_to_spectrogram(data):\n",
        "    waveform, sample_rate = data\n",
        "    spectrogram = mel_spectrogram_transform(waveform)  # Apply the spectrogram transformation\n",
        "    return spectrogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "khV1u_wUIR-o"
      },
      "outputs": [],
      "source": [
        "# post spectrogram augmentations\n",
        "\n",
        "# Example augmentations, could add more\n",
        "time_mask = T.TimeMasking(time_mask_param=10)\n",
        "\n",
        "freq_mask = T.FrequencyMasking(freq_mask_param=8)\n",
        "\n",
        "# hybridizes two sounds\n",
        "def mixup(spectrogram1, spectrogram2, alpha=0.2):\n",
        "    lam = torch.FloatTensor(1).uniform_(0, alpha).item()\n",
        "    return lam * spectrogram1 + (1 - lam) * spectrogram2\n",
        "\n",
        "# should probably implement a randomization process like above\n",
        "def augment_spectrogram(spectrogram):\n",
        "    augmented = time_mask(spectrogram)  # Apply time masking\n",
        "    augmented = freq_mask(augmented)   # Apply frequency masking\n",
        "    return augmented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2U9n6Z-fPiwY"
      },
      "outputs": [],
      "source": [
        "# Decode audio files\n",
        "def decode_audio(file_tuple):\n",
        "    file_path, file = file_tuple\n",
        "    waveform, sample_rate = torchaudio.load(file_path)\n",
        "    return waveform, sample_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "\n",
        "class UrbanSoundDataset(Dataset):\n",
        "    def __init__(self, audio_path, fold, csv_path, transform=None):\n",
        "        self.audio_path = os.path.join(audio_path, f\"fold{fold}\")\n",
        "        self.file_list = [os.path.join(self.audio_path, f) for f in os.listdir(self.audio_path) if f.endswith(\".wav\")]\n",
        "        self.transform = transform\n",
        "\n",
        "        # Load the metadata CSV file\n",
        "        self.metadata = pd.read_csv(csv_path)\n",
        "\n",
        "    def get_label(self, file_name):\n",
        "        \"\"\"Fetch the class label for a given file name from the metadata.\"\"\"\n",
        "        label_row = self.metadata.loc[self.metadata['slice_file_name'] == file_name, 'class']\n",
        "        if not label_row.empty:\n",
        "            return label_row.values[0]\n",
        "        else:\n",
        "            raise ValueError(f\"File name {file_name} not found in metadata CSV.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load the audio file\n",
        "        file_path = self.file_list[idx]\n",
        "        waveform, sample_rate = torchaudio.load(file_path)\n",
        "\n",
        "        # Convert mono to stereo if necessary\n",
        "        if waveform.size(0) == 1:  # If mono\n",
        "            waveform = waveform.repeat(2, 1)\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            waveform = self.transform(waveform)\n",
        "\n",
        "        # Extract the file name from the path\n",
        "        file_name = os.path.basename(file_path)\n",
        "\n",
        "        # Get the corresponding label for the file\n",
        "        label = self.get_label(file_name)\n",
        "\n",
        "        return waveform, label\n",
        "\n",
        "# class UrbanSoundDataset(Dataset):\n",
        "#     def __init__(self, audio_path, fold, transform=None):\n",
        "#         self.audio_path = os.path.join(audio_path, f\"fold{fold}\")\n",
        "#         self.norm_path = os.path.normpath(self.audio_path)\n",
        "#         self.file_list = [os.path.join(self.norm_path, f) for f in os.listdir(self.norm_path) if f.endswith(\".wav\")]\n",
        "#         self.transform = transform\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.file_list)\n",
        "\n",
        "#     # def __getitem__(self, idx):\n",
        "#     #     # Load the audio file\n",
        "#     #     file_path = self.file_list[idx]\n",
        "#     #     waveform, sample_rate = torchaudio.load(file_path)\n",
        "\n",
        "#     #     # Convert mono to stereo if necessary\n",
        "#     #     if waveform.size(0) == 1:\n",
        "#     #         waveform = waveform.repeat(2, 1)\n",
        "\n",
        "        \n",
        "#     #     # Apply any transformations (e.g., augmentations, spectrogram)\n",
        "#     #     if self.transform:\n",
        "#     #         waveform = self.transform(waveform)\n",
        "        \n",
        "#     #     return waveform\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#     file_path = self.file_list[idx]\n",
        "#     waveform, sample_rate = torchaudio.load(file_path)\n",
        "    \n",
        "#     # Convert mono to stereo if necessary\n",
        "#     if waveform.size(0) == 1:\n",
        "#         waveform = waveform.repeat(2, 1)\n",
        "    \n",
        "#     # Apply transformations\n",
        "#     if self.transform:\n",
        "#         waveform = self.transform(waveform)\n",
        "\n",
        "#     # Make sure to return both X (waveform) and y (label)\n",
        "#     label = self.get_label(file_path)  # Replace with your method to get labels\n",
        "#     return waveform, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\asm2f\\Data Science\\DS6050\\UrbanAdversary\\urbad\\Lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torchaudio.transforms as T\n",
        "\n",
        "# Example transformations\n",
        "def augment_waveform(waveform):\n",
        "    # Add your augmentation logic here (e.g., noise addition, time stretch, etc.)\n",
        "    return waveform\n",
        "\n",
        "waveform_to_spectrogram = T.MelSpectrogram(sample_rate=16000, n_mels=128)\n",
        "augment_spectrogram = T.AmplitudeToDB()\n",
        "\n",
        "# Combine transformations into a callable function\n",
        "def transform_pipeline(waveform):\n",
        "    waveform = augment_waveform(waveform)\n",
        "    spectrogram = waveform_to_spectrogram(waveform)\n",
        "    spectrogram = augment_spectrogram(spectrogram)\n",
        "    return spectrogram\n",
        "\n",
        "def pad_with_noise(spectrogram, max_time, noise_std=0.01):\n",
        "    \"\"\"\n",
        "    Pads a spectrogram with Gaussian noise instead of zeros.\n",
        "\n",
        "    Args:\n",
        "        spectrogram (Tensor): Shape (channels, freq_bins, time_steps)\n",
        "        max_time (int): Target time dimension\n",
        "        noise_std (float): Standard deviation of the Gaussian noise\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Padded spectrogram with noise\n",
        "    \"\"\"\n",
        "    # Compute how much padding is needed\n",
        "    pad_amount = max_time - spectrogram.size(2)\n",
        "    \n",
        "    if pad_amount > 0:\n",
        "        # Generate random noise matching the shape of missing time steps\n",
        "        noise = torch.randn((spectrogram.size(0), spectrogram.size(1), pad_amount)) * noise_std\n",
        "        \n",
        "        # Concatenate noise along the time axis\n",
        "        spectrogram = torch.cat([spectrogram, noise], dim=2)\n",
        "    \n",
        "    return spectrogram\n",
        "\n",
        "def convert_to_three_channels(spectrogram):\n",
        "    # Convert [2, 224, 224] to [3, 224, 224]\n",
        "    if spectrogram.size(0) == 2:\n",
        "        # Duplicate the first channel to create a third channel\n",
        "        return torch.cat((spectrogram, spectrogram[0:1, :, :]), dim=0)\n",
        "    return spectrogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchvision\n",
        "\n",
        "class densenet(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    DenseNet Class, derived from Pytorch. Intended for model manipulation (i.e. unfreezing layers, etc.)\n",
        "    To use model, try (densenet).model(data)\n",
        "    May change to reflect manual implementation of densenet161.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()  # Initialize the nn.Module base class\n",
        "        self.model = torchvision.models.densenet161()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)  # Delegate forward pass to the original DenseNet\n",
        "\n",
        "    def layer_change(self):\n",
        "        \"\"\"\n",
        "        Unfreeze layers of DenseNet model per specifications\n",
        "        \"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define training and testing loops\n",
        "def train_loop(dataloader, model, loss_fn, optimizer, epochs=1):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "        size = len(dataloader.dataset)\n",
        "        total_loss = 0  # Initialize variable to accumulate loss per epoch\n",
        "\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            # Compute prediction and loss\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            pred = model(X)\n",
        "            loss = loss_fn(pred, y)\n",
        "\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if batch % 20 == 0:\n",
        "                current = (batch + 1) * len(X)\n",
        "                print(f\"loss: {loss.item():>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "        # Average loss for this epoch\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Training Loss (Epoch): {avg_loss:>7f}\")\n",
        "    return avg_loss  # Return the average loss for the last epoch\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    # Average loss and accuracy for this fold\n",
        "    avg_test_loss = test_loss / num_batches\n",
        "    accuracy = correct / size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {avg_test_loss:>8f} \\n\")\n",
        "    return avg_test_loss, accuracy  # Return both average loss and accuracy for this fold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BLCzmvxcHvKs"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Resize and normalize for DenseNet\n",
        "resize_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize for DenseNet\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standard ImageNet normalization\n",
        "])\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    inputs, labels = zip(*batch)  # Separate inputs and labels\n",
        "    max_time = max(spectrogram.size(2) for spectrogram in inputs)\n",
        "\n",
        "    # Pad inputs to the same length along the time dimension\n",
        "    padded_inputs = [\n",
        "        torch.nn.functional.pad(input, (0, max_time - input.size(2)))\n",
        "        for input in inputs\n",
        "    ]\n",
        "\n",
        "    # Convert to 3 channels and resize\n",
        "    resized_inputs = [resize_transform(convert_to_three_channels(input)) for input in padded_inputs]\n",
        "    \n",
        "    # Map labels to numeric class IDs\n",
        "    class_mapping = {\n",
        "        \"air_conditioner\": 0,\n",
        "        \"car_horn\": 1,\n",
        "        \"children_playing\": 2,\n",
        "        \"dog_bark\": 3,\n",
        "        \"drilling\": 4,\n",
        "        \"engine_idling\": 5,\n",
        "        \"gun_shot\": 6,\n",
        "        \"jackhammer\": 7,\n",
        "        \"siren\": 8,\n",
        "        \"street_music\": 9\n",
        "    }\n",
        "\n",
        "    numeric_labels = [class_mapping[label] for label in labels]\n",
        "\n",
        "    # Stack inputs and labels\n",
        "    return torch.stack(resized_inputs), torch.tensor(numeric_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing Fold 1\n",
            "Epoch 1/1\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     35\u001b[39m val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m, collate_fn=custom_collate_fn)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Train and validate\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m train_loss = \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m val_loss, val_accuracy = test_loop(val_dataloader, model, loss_fn)\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Aggregate metrics\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mtrain_loop\u001b[39m\u001b[34m(dataloader, model, loss_fn, optimizer, epochs)\u001b[39m\n\u001b[32m     12\u001b[39m X = X.to(device)\n\u001b[32m     13\u001b[39m y = y.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m pred = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m loss = loss_fn(pred, y)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asm2f\\Data Science\\DS6050\\UrbanAdversary\\urbad\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asm2f\\Data Science\\DS6050\\UrbanAdversary\\urbad\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mdensenet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asm2f\\Data Science\\DS6050\\UrbanAdversary\\urbad\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asm2f\\Data Science\\DS6050\\UrbanAdversary\\urbad\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asm2f\\Data Science\\DS6050\\UrbanAdversary\\urbad\\Lib\\site-packages\\torchvision\\models\\densenet.py:213\u001b[39m, in \u001b[36mDenseNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m     out = F.relu(features, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    215\u001b[39m     out = F.adaptive_avg_pool2d(out, (\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m))\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asm2f\\Data Science\\DS6050\\UrbanAdversary\\urbad\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asm2f\\Data Science\\DS6050\\UrbanAdversary\\urbad\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asm2f\\Data Science\\DS6050\\UrbanAdversary\\urbad\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asm2f\\Data Science\\DS6050\\UrbanAdversary\\urbad\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asm2f\\Data Science\\DS6050\\UrbanAdversary\\urbad\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asm2f\\Data Science\\DS6050\\UrbanAdversary\\urbad\\Lib\\site-packages\\torchvision\\models\\densenet.py:122\u001b[39m, in \u001b[36m_DenseBlock.forward\u001b[39m\u001b[34m(self, init_features)\u001b[39m\n\u001b[32m    120\u001b[39m features = [init_features]\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.items():\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     new_features = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m     features.append(new_features)\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(features, \u001b[32m1\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asm2f\\Data Science\\DS6050\\UrbanAdversary\\urbad\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asm2f\\Data Science\\DS6050\\UrbanAdversary\\urbad\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asm2f\\Data Science\\DS6050\\UrbanAdversary\\urbad\\Lib\\site-packages\\torchvision\\models\\densenet.py:90\u001b[39m, in \u001b[36m_DenseLayer.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     88\u001b[39m     bottleneck_output = \u001b[38;5;28mself\u001b[39m.bn_function(prev_features)\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m new_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrelu2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbottleneck_output\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.drop_rate > \u001b[32m0\u001b[39m:\n\u001b[32m     92\u001b[39m     new_features = F.dropout(new_features, p=\u001b[38;5;28mself\u001b[39m.drop_rate, training=\u001b[38;5;28mself\u001b[39m.training)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asm2f\\Data Science\\DS6050\\UrbanAdversary\\urbad\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asm2f\\Data Science\\DS6050\\UrbanAdversary\\urbad\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asm2f\\Data Science\\DS6050\\UrbanAdversary\\urbad\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asm2f\\Data Science\\DS6050\\UrbanAdversary\\urbad\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "\n",
        "# Specify paths and batch size\n",
        "AUDIO_PATH = \"./UrbanSound8k/audio\"\n",
        "CSV_PATH = \"./UrbanSound8k/metadata/UrbanSound8K.csv\"\n",
        "batch_size = 32\n",
        "epochs = 1\n",
        "\n",
        "\n",
        "fold_losses = []\n",
        "fold_accuracies = []\n",
        "\n",
        "model = densenet()\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "\n",
        "# Loop through folds\n",
        "# Variables to accumulate metrics across folds\n",
        "fold_losses = []\n",
        "fold_accuracies = []\n",
        "\n",
        "# Loop through folds\n",
        "for fold in range(1, 11):\n",
        "    print(f\"Processing Fold {fold}\")\n",
        "\n",
        "    # Initialize dataset and DataLoader\n",
        "    dataset = UrbanSoundDataset(audio_path=AUDIO_PATH, fold=fold, transform=transform_pipeline, csv_path=CSV_PATH)\n",
        "    train_indices, val_indices = train_test_split(list(range(len(dataset))), test_size=0.2, random_state=666)\n",
        "    \n",
        "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
        "    val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
        "    \n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
        "\n",
        "    # Train and validate\n",
        "    train_loss = train_loop(train_dataloader, model, loss_fn, optimizer, epochs=epochs)\n",
        "    val_loss, val_accuracy = test_loop(val_dataloader, model, loss_fn)\n",
        "\n",
        "    # Aggregate metrics\n",
        "    fold_losses.append(val_loss)\n",
        "    fold_accuracies.append(val_accuracy)\n",
        "\n",
        "# Compute average loss and accuracy across folds\n",
        "mean_loss = sum(fold_losses) / len(fold_losses)\n",
        "mean_accuracy = sum(fold_accuracies) / len(fold_accuracies)\n",
        "\n",
        "print(f\"\\nCross-Validation Results:\")\n",
        "print(f\"Avg Loss: {mean_loss:.6f}, Avg Accuracy: {(100 * mean_accuracy):.2f}%\")\n",
        "\n",
        "\n",
        "    # for i in range(len(batch)):\n",
        "\n",
        "    #     spectrogram = batch[i]\n",
        "\n",
        "    #     # Convert to numpy\n",
        "    #     spectrogram_np = spectrogram.numpy()  # Shape: (2, Freq, Time)\n",
        "\n",
        "    #     # Plot left and right channels\n",
        "    #     fig, axs = plt.subplots(2, 1, figsize=(6, 6), constrained_layout=True)\n",
        "\n",
        "    #     axs[0].imshow(spectrogram_np[0], aspect='auto', origin='lower', cmap='magma')\n",
        "    #     axs[0].set_title(f\"Spectrogram {i+1} - Left Channel\")\n",
        "    #     axs[0].set_ylabel(\"Frequency Bins\")\n",
        "    #     axs[0].set_xlabel(\"Time Frames\")\n",
        "\n",
        "    #     axs[1].imshow(spectrogram_np[1], aspect='auto', origin='lower', cmap='magma')\n",
        "    #     axs[1].set_title(f\"Spectrogram {i+1} - Right Channel\")\n",
        "    #     axs[1].set_ylabel(\"Frequency Bins\")\n",
        "    #     axs[1].set_xlabel(\"Time Frames\")\n",
        "\n",
        "    #     plt.show()\n",
        "\n",
        "# Compute average loss and accuracy across folds\n",
        "mean_loss = sum(fold_losses) / len(fold_losses)\n",
        "mean_accuracy = sum(fold_accuracies) / len(fold_accuracies)\n",
        "\n",
        "print(f\"\\nCross-Validation Results:\")\n",
        "print(f\"Avg Loss: {mean_loss:.6f}, Avg Accuracy: {(100 * mean_accuracy):.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape: torch.Size([2, 176400]), Sample Rate: 44100\n"
          ]
        }
      ],
      "source": [
        "# import torchaudio\n",
        "\n",
        "# audio_path = \"./UrbanSound8k/audio/fold1/137156-9-0-30.wav\"\n",
        "# waveform, sample_rate = torchaudio.load(audio_path)\n",
        "# print(f\"Shape: {waveform.shape}, Sample Rate: {sample_rate}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "urbad",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
