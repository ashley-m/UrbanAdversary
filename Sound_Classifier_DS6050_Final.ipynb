{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mBSFTJ5M_z-H"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import torch\n",
        "from torch.cuda import manual_seed_all\n",
        "from torch.backends import cudnn\n",
        "import matplotlib as mpl\n",
        "from matplotlib import pyplot as plt\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3uDKfjRHMuxk"
      },
      "outputs": [],
      "source": [
        "# pre spectrogram augmentations\n",
        "# these are examples and can be changed based on domain knowledge\n",
        "\n",
        "time_stretch = T.TimeStretch()\n",
        "def stretch_waveform(waveform, rate=1.2):\n",
        "    # `rate > 1.0` speeds up, `rate < 1.0` slows down\n",
        "    return time_stretch(waveform, rate)\n",
        "\n",
        "pitch_shift = T.PitchShift(sample_rate=44100, n_steps=2)  # Shift up by 2 semitones\n",
        "def shift_pitch(waveform, sample_rate):\n",
        "    return pitch_shift(waveform)\n",
        "\n",
        "def scale_volume(waveform, factor=1.5):\n",
        "    return waveform * factor  # Amplifies waveform by factor\n",
        "\n",
        "def crop_waveform(waveform, crop_size):\n",
        "    start = torch.randint(0, max(1, waveform.size(-1) - crop_size), (1,)).item()\n",
        "    return waveform[:, start:start + crop_size]\n",
        "\n",
        "def apply_reverb(waveform):\n",
        "    reverb = T.Reverberate()\n",
        "    return reverb(waveform)\n",
        "\n",
        "def time_shift(waveform, shift):\n",
        "    return torch.roll(waveform, shifts=shift, dims=-1)\n",
        "\n",
        "def add_noise(waveform, noise_level=0.005):\n",
        "    noise = torch.randn_like(waveform) * noise_level\n",
        "    return waveform + noise\n",
        "\n",
        "# Augment on-the-fly stochastically\n",
        "# again these are just examples and do not necessarily utilize the methods above\n",
        "def augment_waveform(data):\n",
        "    waveform, sample_rate = data\n",
        "    if torch.rand(1).item() > 0.5:\n",
        "        waveform += torch.randn_like(waveform) * 0.005\n",
        "    if torch.rand(1).item() > 0.5:\n",
        "        waveform = torch.roll(waveform, shifts=torch.randint(-5000, 5000, (1,)).item(), dims=-1)\n",
        "    if torch.rand(1).item() > 0.5:\n",
        "        waveform *= torch.FloatTensor(1).uniform_(0.8, 1.5).item()\n",
        "    return waveform, sample_rate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ww8OMV8nNZcf"
      },
      "outputs": [],
      "source": [
        "# Create a MelSpectrogram transformation\n",
        "mel_spectrogram_transform = T.MelSpectrogram(\n",
        "    sample_rate=44100,         # Default sample rate, change if needed\n",
        "    n_fft=1024,                # Number of FFT bins\n",
        "    hop_length=512,            # Hop length between windows\n",
        "    n_mels=64                  # Number of Mel bands\n",
        ")\n",
        "\n",
        "def waveform_to_spectrogram(data):\n",
        "    waveform, sample_rate = data\n",
        "    spectrogram = mel_spectrogram_transform(waveform)  # Apply the spectrogram transformation\n",
        "    return spectrogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "khV1u_wUIR-o"
      },
      "outputs": [],
      "source": [
        "# post spectrogram augmentations\n",
        "\n",
        "# Example augmentations, could add more\n",
        "time_mask = T.TimeMasking(time_mask_param=10)\n",
        "\n",
        "freq_mask = T.FrequencyMasking(freq_mask_param=8)\n",
        "\n",
        "# hybridizes two sounds\n",
        "def mixup(spectrogram1, spectrogram2, alpha=0.2):\n",
        "    lam = torch.FloatTensor(1).uniform_(0, alpha).item()\n",
        "    return lam * spectrogram1 + (1 - lam) * spectrogram2\n",
        "\n",
        "# should probably implement a randomization process like above\n",
        "def augment_spectrogram(spectrogram):\n",
        "    augmented = time_mask(spectrogram)  # Apply time masking\n",
        "    augmented = freq_mask(augmented)   # Apply frequency masking\n",
        "    return augmented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2U9n6Z-fPiwY"
      },
      "outputs": [],
      "source": [
        "# Decode audio files\n",
        "def decode_audio(file_tuple):\n",
        "    file_path, file = file_tuple\n",
        "    waveform, sample_rate = torchaudio.load(file_path)\n",
        "    return waveform, sample_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "\n",
        "class UrbanSoundDataset(Dataset):\n",
        "    def __init__(self, audio_path, fold, csv_path, transform=None):\n",
        "        self.audio_path = os.path.join(audio_path, f\"fold{fold}\")\n",
        "        self.file_list = [os.path.join(self.audio_path, f) for f in os.listdir(self.audio_path) if f.endswith(\".wav\")]\n",
        "        self.transform = transform\n",
        "\n",
        "        # Load the metadata CSV file\n",
        "        self.metadata = pd.read_csv(csv_path)\n",
        "\n",
        "    def get_label(self, file_name):\n",
        "        \"\"\"Fetch the class label for a given file name from the metadata.\"\"\"\n",
        "        label_row = self.metadata.loc[self.metadata['slice_file_name'] == file_name, 'class']\n",
        "        if not label_row.empty:\n",
        "            return label_row.values[0]\n",
        "        else:\n",
        "            raise ValueError(f\"File name {file_name} not found in metadata CSV.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load the audio file\n",
        "        file_path = self.file_list[idx]\n",
        "        waveform, sample_rate = torchaudio.load(file_path)\n",
        "\n",
        "        # Convert mono to stereo if necessary\n",
        "        if waveform.size(0) == 1:  # If mono\n",
        "            waveform = waveform.repeat(2, 1)\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            waveform = self.transform(waveform)\n",
        "\n",
        "        # Extract the file name from the path\n",
        "        file_name = os.path.basename(file_path)\n",
        "\n",
        "        # Get the corresponding label for the file\n",
        "        label = self.get_label(file_name)\n",
        "\n",
        "        return waveform, label\n",
        "\n",
        "# class UrbanSoundDataset(Dataset):\n",
        "#     def __init__(self, audio_path, fold, transform=None):\n",
        "#         self.audio_path = os.path.join(audio_path, f\"fold{fold}\")\n",
        "#         self.norm_path = os.path.normpath(self.audio_path)\n",
        "#         self.file_list = [os.path.join(self.norm_path, f) for f in os.listdir(self.norm_path) if f.endswith(\".wav\")]\n",
        "#         self.transform = transform\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.file_list)\n",
        "\n",
        "#     # def __getitem__(self, idx):\n",
        "#     #     # Load the audio file\n",
        "#     #     file_path = self.file_list[idx]\n",
        "#     #     waveform, sample_rate = torchaudio.load(file_path)\n",
        "\n",
        "#     #     # Convert mono to stereo if necessary\n",
        "#     #     if waveform.size(0) == 1:\n",
        "#     #         waveform = waveform.repeat(2, 1)\n",
        "\n",
        "        \n",
        "#     #     # Apply any transformations (e.g., augmentations, spectrogram)\n",
        "#     #     if self.transform:\n",
        "#     #         waveform = self.transform(waveform)\n",
        "        \n",
        "#     #     return waveform\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#     file_path = self.file_list[idx]\n",
        "#     waveform, sample_rate = torchaudio.load(file_path)\n",
        "    \n",
        "#     # Convert mono to stereo if necessary\n",
        "#     if waveform.size(0) == 1:\n",
        "#         waveform = waveform.repeat(2, 1)\n",
        "    \n",
        "#     # Apply transformations\n",
        "#     if self.transform:\n",
        "#         waveform = self.transform(waveform)\n",
        "\n",
        "#     # Make sure to return both X (waveform) and y (label)\n",
        "#     label = self.get_label(file_path)  # Replace with your method to get labels\n",
        "#     return waveform, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\asm2f\\Data Science\\DS6050\\UrbanAdversary\\urbad\\Lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torchaudio.transforms as T\n",
        "\n",
        "# Example transformations\n",
        "def augment_waveform(waveform):\n",
        "    # Add your augmentation logic here (e.g., noise addition, time stretch, etc.)\n",
        "    return waveform\n",
        "\n",
        "waveform_to_spectrogram = T.MelSpectrogram(sample_rate=16000, n_mels=128)\n",
        "augment_spectrogram = T.AmplitudeToDB()\n",
        "\n",
        "# Combine transformations into a callable function\n",
        "def transform_pipeline(waveform):\n",
        "    waveform = augment_waveform(waveform)\n",
        "    spectrogram = waveform_to_spectrogram(waveform)\n",
        "    spectrogram = augment_spectrogram(spectrogram)\n",
        "    return spectrogram\n",
        "\n",
        "def pad_with_noise(spectrogram, max_time, noise_std=0.01):\n",
        "    \"\"\"\n",
        "    Pads a spectrogram with Gaussian noise instead of zeros.\n",
        "\n",
        "    Args:\n",
        "        spectrogram (Tensor): Shape (channels, freq_bins, time_steps)\n",
        "        max_time (int): Target time dimension\n",
        "        noise_std (float): Standard deviation of the Gaussian noise\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Padded spectrogram with noise\n",
        "    \"\"\"\n",
        "    # Compute how much padding is needed\n",
        "    pad_amount = max_time - spectrogram.size(2)\n",
        "    \n",
        "    if pad_amount > 0:\n",
        "        # Generate random noise matching the shape of missing time steps\n",
        "        noise = torch.randn((spectrogram.size(0), spectrogram.size(1), pad_amount)) * noise_std\n",
        "        \n",
        "        # Concatenate noise along the time axis\n",
        "        spectrogram = torch.cat([spectrogram, noise], dim=2)\n",
        "    \n",
        "    return spectrogram\n",
        "\n",
        "def convert_to_three_channels(spectrogram):\n",
        "    # Convert [2, 224, 224] to [3, 224, 224]\n",
        "    if spectrogram.size(0) == 2:\n",
        "        # Duplicate the first channel to create a third channel\n",
        "        return torch.cat((spectrogram, spectrogram[0:1, :, :]), dim=0)\n",
        "    return spectrogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchvision\n",
        "\n",
        "class densenet(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    DenseNet Class, derived from Pytorch. Intended for model manipulation (i.e. unfreezing layers, etc.)\n",
        "    To use model, try (densenet).model(data)\n",
        "    May change to reflect manual implementation of densenet161.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()  # Initialize the nn.Module base class\n",
        "        self.model = torchvision.models.densenet161()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)  # Delegate forward pass to the original DenseNet\n",
        "\n",
        "    def layer_change(self):\n",
        "        \"\"\"\n",
        "        Unfreeze layers of DenseNet model per specifications\n",
        "        \"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define training and testing loops\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  \n",
        "  size = len(dataloader.dataset)\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    # Compute prediction and loss\n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "    pred = model(X)\n",
        "    global loss # for future retrieval\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch % 20 == 0:\n",
        "        loss, current = loss.item(), (batch + 1) * len(X)\n",
        "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  global test_loss # for future retrieval\n",
        "  test_loss, correct = 0, 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLCzmvxcHvKs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss: 6.908273  [   32/  873]\n"
          ]
        }
      ],
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Resize and normalize for DenseNet\n",
        "resize_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize for DenseNet\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standard ImageNet normalization\n",
        "])\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    inputs, labels = zip(*batch)  # Separate inputs and labels\n",
        "    max_time = max(spectrogram.size(2) for spectrogram in inputs)\n",
        "\n",
        "    # Pad inputs to the same length along the time dimension\n",
        "    padded_inputs = [\n",
        "        torch.nn.functional.pad(input, (0, max_time - input.size(2)))\n",
        "        for input in inputs\n",
        "    ]\n",
        "\n",
        "    # Convert to 3 channels and resize\n",
        "    resized_inputs = [resize_transform(convert_to_three_channels(input)) for input in padded_inputs]\n",
        "    \n",
        "    # Map labels to numeric class IDs\n",
        "    class_mapping = {\n",
        "        \"air_conditioner\": 0,\n",
        "        \"car_horn\": 1,\n",
        "        \"children_playing\": 2,\n",
        "        \"dog_bark\": 3,\n",
        "        \"drilling\": 4,\n",
        "        \"engine_idling\": 5,\n",
        "        \"gun_shot\": 6,\n",
        "        \"jackhammer\": 7,\n",
        "        \"siren\": 8,\n",
        "        \"street_music\": 9\n",
        "    }\n",
        "\n",
        "    numeric_labels = [class_mapping[label] for label in labels]\n",
        "\n",
        "    # Stack inputs and labels\n",
        "    return torch.stack(resized_inputs), torch.tensor(numeric_labels)\n",
        "\n",
        "# Specify paths and batch size\n",
        "AUDIO_PATH = \"./UrbanSound8k/audio\"\n",
        "CSV_PATH = \"./UrbanSound8k/metadata/UrbanSound8K.csv\"\n",
        "batch_size = 32\n",
        "\n",
        "target_shape = [2, 128, 1024]  # Adjust time_frames as needed\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "loss = []\n",
        "accuracy = []\n",
        "model = densenet()\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "\n",
        "# Loop through folds\n",
        "for fold in range(1, 11):\n",
        "    # Initialize dataset and DataLoader\n",
        "    dataset = UrbanSoundDataset(audio_path=AUDIO_PATH, fold=fold, transform=transform_pipeline, csv_path=CSV_PATH)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
        "\n",
        "    train_loop(dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(dataloader, model, loss_fn, optimizer)\n",
        "    # Process each batch\n",
        "    # for batch in dataloader:\n",
        "        # Display only the first 10 spectrograms\n",
        "    # for i in range(len(batch)):\n",
        "\n",
        "    #     spectrogram = batch[i]\n",
        "\n",
        "    #     # Convert to numpy\n",
        "    #     spectrogram_np = spectrogram.numpy()  # Shape: (2, Freq, Time)\n",
        "\n",
        "    #     # Plot left and right channels\n",
        "    #     fig, axs = plt.subplots(2, 1, figsize=(6, 6), constrained_layout=True)\n",
        "\n",
        "    #     axs[0].imshow(spectrogram_np[0], aspect='auto', origin='lower', cmap='magma')\n",
        "    #     axs[0].set_title(f\"Spectrogram {i+1} - Left Channel\")\n",
        "    #     axs[0].set_ylabel(\"Frequency Bins\")\n",
        "    #     axs[0].set_xlabel(\"Time Frames\")\n",
        "\n",
        "    #     axs[1].imshow(spectrogram_np[1], aspect='auto', origin='lower', cmap='magma')\n",
        "    #     axs[1].set_title(f\"Spectrogram {i+1} - Right Channel\")\n",
        "    #     axs[1].set_ylabel(\"Frequency Bins\")\n",
        "    #     axs[1].set_xlabel(\"Time Frames\")\n",
        "\n",
        "    #     plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape: torch.Size([2, 176400]), Sample Rate: 44100\n"
          ]
        }
      ],
      "source": [
        "import torchaudio\n",
        "\n",
        "audio_path = \"./UrbanSound8k/audio/fold1/137156-9-0-30.wav\"\n",
        "waveform, sample_rate = torchaudio.load(audio_path)\n",
        "print(f\"Shape: {waveform.shape}, Sample Rate: {sample_rate}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "urbad",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
